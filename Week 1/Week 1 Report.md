# **Week 1 Report : Data Analysis & Feature Engineering**

---

This document details the complete analysis performed during Week 1 of the project, corresponding to the "Project Planning, Importing, Pre-Processing, Data Visualization & Data Modelling" milestone. 

All code and analysis described here can be found in the Analysis.ipynb Jupyter Notebook. 

## **1\. About the Dataset** 

* The foundation of this project is the Dataset.csv file, which contains hour-by-hour simulation data from a hybrid microgrid's operation. 

**Source :** Sourced from [Kaggle](https://www.kaggle.com/). 

**Download Link :** [Renewable Energy Microgrid Dataset](https://www.kaggle.com/datasets/programmer3/renewable-energy-microgrid-dataset) 

Dataset Column Breakdown 

* The dataset provides comprehensive information on the weather, energy generated, user demand, and the grid's status. 

### **A. Core Identifiers** 

* **timestamp :** The date and time of the reading (hourly frequency). 

### **B. Target Variables (What We Need to Predict)** 

* **solar\_output :** (in MW) The actual power generated by the solar panels.   
* **wind\_output :** (in MW) The actual power generated by the wind turbines.   
* **load\_demand :** (in MW) The total electricity consumed by the users on the microgrid. 

### **C. Weather Features (Our Predictors)** 

* **solar\_irradiance :** (in W/m²) The amount of solar radiation. The primary driver for solar\_output.   
* **wind\_speed :** (in m/s) The speed of the wind. The primary driver for wind\_output.   
* **temperature :** (in °C) Ambient air temperature, which can affect panel efficiency and load.   
* **humidity :** (in %) Relative humidity.   
* **pressure :** (in hPa) Atmospheric pressure. 

### **D. Grid Status Variables (Simulation Outputs)** 

* **grid\_frequency :** (in Hz) The operating frequency of the grid.   
* **grid\_voltage :** (in V) The operating voltage of the grid.   
* **grid\_exchange :** (in MW) Power flow between the microgrid and the main grid.   
* **battery\_soc :** (State of Charge, in %) How full the battery is.   
* **battery\_charge :** (in MW) Power flowing into the battery.   
* **battery\_discharge :** (in MW) Power flowing out of the battery. 

### **E. Pre-Calculated Columns** 

#### **Note :** 

* The provided Dataset.csv also contains columns like predicted\_solar\_output, hour, and day\_of\_week. In our analysis.ipynb notebook, these columns are explicitly dropped to build our own models and create fresh, reliable time-based features from the raw timestamp. 

## **2\. Methodology & Code Analysis (from Analysis.ipynb)** 

* This section provides a detailed, step-by-step walkthrough of the data analysis and preparation process documented in the analysis.ipynb Jupyter Notebook. 

### **Step 2.1 : Environment & Setup** 

* The notebook installs the necessary Python libraries using pip : 

| pip install pandas numpy matplotlib seaborn |
| :---- |

### **Step 2.2 : Library Imports** 

* We begin by importing the foundational libraries for our analysis :   
  * **pandas :** The primary tool for loading and manipulating data (DataFrames).   
  * **numpy :** Used for high-performance numerical operations.   
  * **matplotlib.pyplot & seaborn :** Used for data visualization.   
  * **os :** Used to create a new folder for the processed output.   
  * **pd.set\_option('display.max\_columns', None) :** A helper command to ensure pandas displays all columns. 

### **Step 2.3 : Data Loading and Initial Inspection** 

* With our libraries ready, we load and inspect the dataset. 

#### **A. Load Data :** 

* We use pd.read\_csv('Dataset.csv') to load our data into a DataFrame named df. 

#### **B. Examine Data Structure (df.info()) :** 

* This is our first and most important diagnostic step.   
* **Finding (Entries) :** It shows 3546 entries, providing a partial dataset for analysis.   
* **Finding (Null Values) :** It shows 3546 non-null for all columns. This is excellent, as it means there is no missing data, and we can skip complex imputation steps.   
* **Key Finding (Data Types) :** It reveals that the timestamp column is of type object (a string). This is a critical problem. To perform any time-series analysis, we must convert this column to a proper datetime format.   
* **Key Finding (Existing Features) :** The inspection also reveals that hour and day\_of\_week columns already exist. These will be dropped and recreated from the timestamp to ensure accuracy. 

#### **C. Statistical Summary (df.describe()) :** 

* This command provides a statistical overview (mean, median, min, max) of all numerical columns.   
* **Purpose :** This is a "sanity check" to spot anomalies, such as negative values for wind\_speed or solar\_irradiance (none were found). 

### **Step 2.4 : Data Cleaning and Preparation** 

* Before analysis, the data is cleaned and prepared for time-series modeling.   
* **Timestamp Conversion :** We convert the timestamp column using pd.to\_datetime(df\['timestamp'\]).   
* **Set Index :** The timestamp column is set as the DataFrame's index, which is standard practice for time-series analysis.   
* **Drop Unneeded Columns :**  We drop the pre-calculated or redundant columns identified in Step 2.3.B : total\_renewable, predicted\_solar\_output, predicted\_wind\_output, predicted\_total\_energy, hour, and day\_of\_week. 

### **Step 2.5 : Feature Engineering** 

* This is the most critical step for a time-series model. We create new, informative features from the existing data. 

#### **A. Temporal Features (Extracting Cycles) :** 

* We "break down" the new datetime index into numerical features that a machine learning model can understand :   
  * df\['hour'\] \= df.index.hour (A number from 0 to 23\)   
  * df\['day\_of\_week'\] \= df.index.dayofweek (0=Monday, 6=Sunday)    
  * df\['month'\] \= df.index.month (A number from 1 to 12\)   
  * df\['day\_of\_year'\] \= df.index.dayofyear (A number from 1 to 365\)   
* **Why :** This explicitly tells the model about 24-hour daily cycles (e.g., hour 12 is always high-sun) and 12-month seasonal cycles. 

#### **B. Lag Features (Autoregression) :** 

* We create new features like load\_demand\_lag1 \= df\['load\_demand'\].shift(1).   
* **Why :** This is a core concept of time-series forecasting. shift(1) creates a new column where the value for each hour is the load\_demand from the previous hour. This is based on the strong assumption that the energy demand at 9 AM is an excellent predictor of the demand at 10 AM. Lag features were created for load\_demand, solar\_irradiance, and wind\_speed. 

#### **C. Rolling Averages (Smoothing) :** 

* We create features like wind\_speed\_roll\_3h \= df\['wind\_speed'\].rolling(window=3).mean().   
* **Why :** This smooths out short-term, noisy fluctuations. The 3-hour average wind speed is a more stable and robust feature, representing the general trend. Rolling averages were created for wind\_speed, solar\_irradiance, and load\_demand. 

### **Step 2.6: Exploratory Data Analysis (EDA)** 

* After cleaning and feature engineering, we visualize the data to discover patterns, test hypotheses, and identify relationships between variables. 

#### **A. Target Variable Behavior Over Time :** 

* We plot our three target variables—load\_demand, solar\_output, and wind\_output—against time.   
* **Insights :** The graph confirms distinct cyclical patterns. Load Demand shows daily peaks and valleys. Solar Output shows a clear daily pattern (zero at night, peaking at noon). Wind Output appears much more stochastic (random) and less predictable. 

![image 1](https://github.com/DhammadeepRamteke/hybrid-microgrid-controller/blob/bc895a28d14c6b6ceb722d53969ad176643fde38/Week%201/target_variables.png)
Target Variable Time-Series (Full Year)

#### **B. Distribution of Key Features :** 

* We plot histograms for our primary predictors (solar\_irradiance, wind\_speed, temperature) and load\_demand.   
* **Insights :** This helps us understand the "shape" of our data. solar\_irradiance clearly shows a "bimodal" distribution, with a large spike at 0 (representing all nighttime hours) and a second peak for daylight hours. 

![](https://github.com/DhammadeepRamteke/hybrid-microgrid-controller/blob/71385edc43ec38e24c5614c1e52f554fbb86611a/Week%201/feature_distributions.png) 
Distribution of Key Features (Histograms)

#### **C. Feature Correlation Heatmap :** 

* We compute the Pearson correlation matrix for all variables and visualize it as a heatmap.   
* **Purpose :** This is crucial for selecting the best predictive features. A value of \+1 means a perfect positive correlation (if one goes up, the other goes up).   
* **Insights :** The heatmap confirms a very high positive correlation between solar\_irradiance and solar\_output, and a strong positive correlation between wind\_speed and wind\_output. This confirms our primary features are excellent predictors. 

![][image3]  
Feature Correlation Heatmap

### **Step 2.7 : Final NaN Handling & Export** 

* **Handling NaN Values :** The shift() and rolling() functions create NaN (Not a Number) values at the beginning of the dataset (e.g., the first row has no "previous hour" to lag from).   
* **Action :** Since machine learning models cannot process NaN values, we drop these first few rows using df.dropna(). This is acceptable as it only removes a tiny fraction of our data.   
* **Export Processed Data :** As the final step, the notebook saves the fully cleaned and feature-engineered DataFrame to a new file, Week 2/processed\_dataset.csv, making it ready for the next phase of the project. 
